
## What is it ?

RapidsMPF is a backend streaming data processing engine using a CSP-style streaming network written in C++ with additional hooks for Python.  Query plans are generated by a frontend (e.g., Polars) and replicated to all workers, each executing independently on its local data partition.  

Execution on each worker is structured as a network of CSP-processes operating in a streaming fashion. 

```
     Source
        |
        v
     Node 1
     /    \
    v      v
 Node 2  node 3
     \    /
      v  v
    Accumulator
        |
        v
      Sink
```

As data becomes available to a CSP-process, it begins computation and forwards results downstream — enabling overlapping of I/O, computation, and communication, and natively supports out-of-core processing.

Here is an ASCII plot illustrating overlapping tasks in a streaming engine:

```
Time --->

scan:     |====|====|====|====|
                \    \    \    \
compute:   .----|====|====|====|====|
                \    \    \    \
sink:      ......|====|====|====|====|

Legend:
|====| : Active period of the task
\    : Data passed downstream as it becomes available
.    : Waiting or not yet started
```

`scan`, `compute`, and `sink` all operate in a pipelined fashion; a data is pushed through the network, the engine makes forward progress on those data,




## RapidsMPF Streaming Network

RapidsMPF builds a network of workers where each worker starts one process/multiple threads and is pinned to one GPU.  Query plans are transformed in a list of nodes (coroutines) where each worker performs a single relations operation on a data chunk at a time

Each process has a set of input channels it reads from and output channels it writes to. For example:

- A parquet-read CSP-process has only an output channel — it generates data chunks.
- A filtering CSP-process has both input and output — it reads data, applies a predicate, and emits filtered results.

We execute a workflow in a streaming fashion, where coroutines are scheduled as soon as input data becomes available, and channel can manage its own internal buffering, backpressure, and remote communication as needed.

Buffers are maintained with a Buffer Manager so that we can move data seamlessly between device and host enabling out-of-core processing and spilling; thus prevent out-of-memory errors when system + device memory is sufficient.

```
+------+     +--------+     +--------+     +------+
| Scan | --> | Select | --> | Filter | --> | Sink |
+------+     +--------+     +--------+     +------+
```

### RapidsMPF Node Types

Nodes fall into two categories:
- Local Nodes: These include operations like filtering, projection, or column-wise transforms. They operate exclusively on local data and preserve CSP semantics.

- Collective Nodes: These include operations like shuffle and join, which require access to non-local data. 

In the case of a collective nodes, remote communication is handled internally – not through channels. For example, a shuffle node may need to access all partitions of a table — both local and remote — but this coordination and data exchange happens inside the CSP-process itself.

This hybrid model, which combines a SPMD-style distribution model and a local CSP-style streaming model, offers several advantages:

- It enables clear process composition and streaming semantics for local operations.

- It allows collective coordination to be localized inside CSP-processes, avoiding the need for global synchronization or a complete global task graph.

- It makes inter-worker parallelism explicit through SPMD-style communication.


## Definitions
- **Network**: A graph of nodes and edges.  `Nodes` are the relational operators on data and edges are the `channels` connecting the _next_ operation in the workflow

- **Context**: Context provides access to resources necessary for executing nodes:
  - UCXX communicators
  - Thread pool executor
  - CUDA Memory (RMM) 
  - RapidMPF Buffer Resource (spillable)

- **Buffer** : Raw Memory buffers typically shared pointers from tabular data provided by cuDF
  - Buffers are created mostly commonly during scan (read_parquet) operations but can also be created during joins and aggregations.  When operating on mulitple buffers either a new stream is created for the new buffer or re-use of an existing stream to attach the newly created buffer

  - Buffers have an attached [CUDA Stream](https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf) maintained for the lifetime of the buffer. 
  - Streams are created or used from an existing stream pool
  
- **Messages**:[Type-erased](https://en.wikipedia.org/wiki/Type_erasure) container for data payloads (shared memory pointers) including: cudf tables, buffers, and RapidsMPF internal data structures like packed data
  - Messages also contain metadata like a sequence number
  - Sequences _do not_ guarantee that chunks arrive in order but they do provide the order in which the data was created


- **Nodes**: Coroutine-based asynchronous relational operator: read, filter, select, join.  
  - Multiple Nodes can be executed concurrently
  - Nodes send/recv data on a channel, process it, and send the result to an output channel
  - Nodes can communicate with each other directly like in the cases of: shuffles, joins, etc.


 <!-- need to write a better pseudo exmaple -->

<table>
<tr>
<th>Python</th>
<th>C++</th>
</tr>
<tr>
<td valign="top">

```python
async def accumulator(ctx: Context, ch_out: Channel, ch_in: Channel):
    """Aggegration Node"""

    total = 0
    while (msg := await ch_in is not None:)
        chunk = TableChunk.from_message(msg)
        total = SUM(chunk.column)
    await send(total)
```

</td>
<td valign="top">

```c++
rapidsmpf::task<void> accumulator(
    std::shared_ptr<rapidsmpf::Channel> ch_out,
    std::shared_ptr<rapidsmpf::Channel> ch_in)
{
    int64_t total = 0;
    while (true) {
        auto msg = co_await ch_in->recv();
        if (!msg) {
            break;
        }
        auto chunk = rapidsmpf::TableChunk::from_message(*msg);

        total += chunk->get_column("value")->sum<int64_t>();
    }

    // Send the accumulated result downstream as a message
    co_await ch_out->send(rapidsmpf::make_message(total));
}
```

</td>
</tr>
</table>


- **Channels**: An asynchronous messaging queue used for forward progressing `messages`.
  - Can be throttled to prevent over production of messages – useful when writing producer nodes that otherwise do not depend on an input channel.
  - Throttling limits the number of concurrent tasks/nodes
Sending suspends when channel is full
