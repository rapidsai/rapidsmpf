/**
 * SPDX-FileCopyrightText: Copyright (c) 2025-2026, NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: Apache-2.0
 */

#include "join.hpp"

#include <algorithm>
#include <memory>
#include <numeric>
#include <vector>

#include <cudf/column/column_view.hpp>
#include <cudf/concatenate.hpp>
#include <cudf/contiguous_split.hpp>
#include <cudf/copying.hpp>
#include <cudf/join/filtered_join.hpp>
#include <cudf/join/hash_join.hpp>
#include <cudf/table/table.hpp>
#include <cudf/table/table_view.hpp>
#include <cudf/types.hpp>
#include <cudf/utilities/span.hpp>
#include <rmm/cuda_stream_view.hpp>

#include <rapidsmpf/communicator/communicator.hpp>
#include <rapidsmpf/cuda_event.hpp>
#include <rapidsmpf/cuda_stream.hpp>
#include <rapidsmpf/error.hpp>
#include <rapidsmpf/integrations/cudf/partition.hpp>
#include <rapidsmpf/memory/packed_data.hpp>
#include <rapidsmpf/shuffler/chunk.hpp>
#include <rapidsmpf/streaming/coll/allgather.hpp>
#include <rapidsmpf/streaming/coll/shuffler.hpp>
#include <rapidsmpf/streaming/core/channel.hpp>
#include <rapidsmpf/streaming/core/context.hpp>
#include <rapidsmpf/streaming/core/node.hpp>
#include <rapidsmpf/streaming/cudf/table_chunk.hpp>

#include "utils.hpp"

namespace rapidsmpf::ndsh {

coro::task<streaming::Message> broadcast(
    std::shared_ptr<streaming::Context> ctx,
    std::shared_ptr<streaming::Channel> ch_in,
    OpID tag,
    streaming::AllGather::Ordered ordered
) {
    streaming::ShutdownAtExit c{ch_in};
    co_await ctx->executor()->schedule();
    CudaEvent event;
    ctx->comm()->logger().print("Broadcast ", static_cast<int>(tag));
    if (ctx->comm()->nranks() == 1) {
        std::vector<streaming::TableChunk> chunks;
        std::vector<cudf::table_view> views;
        auto gather_stream = ctx->br()->stream_pool().get_stream();
        while (true) {
            auto msg = co_await ch_in->receive();
            if (msg.empty()) {
                break;
            }
            auto chunk = to_device(ctx, msg.release<streaming::TableChunk>());
            cuda_stream_join(gather_stream, chunk.stream(), &event);
            views.push_back(chunk.table_view());
            chunks.push_back(std::move(chunk));
        }
        if (chunks.size() == 1) {
            co_return streaming::to_message(
                0, std::make_unique<streaming::TableChunk>(std::move(chunks[0]))
            );
        } else {
            RAPIDSMPF_EXPECTS(chunks.size() > 0, "No chunks in broadcast");
            auto result = cudf::concatenate(views, gather_stream, ctx->br()->device_mr());
            // So that deallocation of the consitutent tables is stream-ordered wrt the
            // concatenation.
            cuda_stream_join(
                chunks
                    | std::views::transform([](auto&& chunk) { return chunk.stream(); }),
                std::ranges::single_view(gather_stream),
                &event
            );
            co_return streaming::to_message(
                0,
                std::make_unique<streaming::TableChunk>(std::move(result), gather_stream)
            );
        }
    } else {
        streaming::AllGather gatherer{ctx, tag};
        while (true) {
            auto msg = co_await ch_in->receive();
            if (msg.empty()) {
                break;
            }
            // TODO: If this chunk is already in pack form, this is unnecessary.
            auto chunk = to_device(ctx, msg.release<streaming::TableChunk>());
            auto pack =
                cudf::pack(chunk.table_view(), chunk.stream(), ctx->br()->device_mr());
            auto packed_data = PackedData(
                std::move(pack.metadata),
                ctx->br()->move(std::move(pack.gpu_data), chunk.stream())
            );
            gatherer.insert(msg.sequence_number(), {std::move(packed_data)});
        }
        gatherer.insert_finished();
        auto result = co_await gatherer.extract_all(ordered);
        if (result.size() == 1) {
            co_return streaming::to_message(
                0,
                std::make_unique<streaming::TableChunk>(
                    std::make_unique<PackedData>(std::move(result[0]))
                )
            );
        } else {
            auto stream = ctx->br()->stream_pool().get_stream();
            co_return streaming::to_message(
                0,
                std::make_unique<streaming::TableChunk>(
                    unpack_and_concat(
                        unspill_partitions(
                            std::move(result), ctx->br().get(), true, ctx->statistics()
                        ),
                        stream,
                        ctx->br().get(),
                        ctx->statistics()
                    ),
                    stream
                )
            );
        }
    }
}

streaming::Node broadcast(
    std::shared_ptr<streaming::Context> ctx,
    std::shared_ptr<streaming::Channel> ch_in,
    std::shared_ptr<streaming::Channel> ch_out,
    OpID tag,
    streaming::AllGather::Ordered ordered
) {
    streaming::ShutdownAtExit c{ch_in, ch_out};
    co_await ctx->executor()->schedule();
    co_await ch_out->send(co_await broadcast(ctx, ch_in, tag, ordered));
    co_await ch_out->drain(ctx->executor());
}

/**
 * @brief Join a table chunk against a build hash table returning a message of the result.
 *
 * @param ctx Streaming context
 * @param left_chunk Chunk to join. Used as the probe table in a filtered join.
 * @param right_chunk Chunk to join. Used as the build table in a filtered join.
 * @param right_on Key column indices in `left_chunk`.
 * @param right_on Key column indices in `right_chunk`.
 * @param sequence Sequence number of the output
 *
 * @return Message of `TableChunk` containing the result of the inner join.
 */
streaming::Message semi_join_chunk(
    std::shared_ptr<streaming::Context> ctx,
    streaming::TableChunk const& left_chunk,
    streaming::TableChunk&& right_chunk,
    [[maybe_unused]] std::vector<cudf::size_type> left_on,
    std::vector<cudf::size_type> right_on,
    std::uint64_t sequence,
    CudaEvent* left_event
) {
    CudaEvent event;  // TODO: see if this is needed for deallocation.
    right_chunk = to_device(ctx, std::move(right_chunk));
    auto chunk_stream = right_chunk.stream();

    left_event->stream_wait(chunk_stream);

    // At this point, both left_chunk and right_chunk are valid on
    // either stream. We'll do everything from here out on the
    // right_chunk.stream(), so that we don't introduce false dependencies
    // between the different chunks.

    auto joiner = cudf::filtered_join(
        right_chunk.table_view().select(right_on),
        cudf::null_equality::UNEQUAL,
        cudf::set_as_build_table::RIGHT,
        chunk_stream
    );

    auto match = joiner.semi_join(
        left_chunk.table_view().select(left_on), chunk_stream, ctx->br()->device_mr()
    );

    ctx->comm()->logger().debug(
        "semi_join_chunk: left.num_rows()=", left_chunk.table_view().num_rows()
    );
    ctx->comm()->logger().debug("semi_join_chunk: match.size()=", match->size());

    cudf::column_view indices = cudf::device_span<cudf::size_type const>(*match);
    auto result_columns = cudf::gather(
                              left_chunk.table_view(),
                              indices,
                              cudf::out_of_bounds_policy::DONT_CHECK,
                              chunk_stream,
                              ctx->br()->device_mr()
    )
                              ->release();

    auto result_table = std::make_unique<cudf::table>(std::move(result_columns));
    ctx->comm()->logger().debug(
        "semi_join_chunk: result_table.num_rows()=", result_table->num_rows()
    );
    // Deallocation of the join indices will happen on chunk_stream, so add stream dep
    cuda_stream_join(left_chunk.stream(), chunk_stream, &event);
    return streaming::to_message(
        sequence,
        std::make_unique<streaming::TableChunk>(std::move(result_table), chunk_stream)
    );
}

/**
 * @brief Join a table chunk against a build hash table returning a message of the result.
 *
 * @param ctx Streaming context
 * @param right_chunk Chunk to join
 * @param sequence Sequence number of the output
 * @param joiner hash_join object, representing the build table.
 * @param build_carrier Columns from the build-side table to be included in the output.
 * @param right_on Key column indiecs in `right_chunk`.
 * @param build_stream Stream the `joiner` will be deallocated on.
 * @param build_event Event recording the creation of the `joiner`.
 *
 * @return Message of `TableChunk` containing the result of the inner join.
 */
streaming::Message inner_join_chunk(
    std::shared_ptr<streaming::Context> ctx,
    streaming::TableChunk&& right_chunk,
    std::uint64_t sequence,
    cudf::hash_join& joiner,
    cudf::table_view build_carrier,
    std::vector<cudf::size_type> right_on,
    rmm::cuda_stream_view build_stream,
    CudaEvent* build_event
) {
    CudaEvent event;
    right_chunk = to_device(ctx, std::move(right_chunk));
    auto chunk_stream = right_chunk.stream();
    build_event->stream_wait(chunk_stream);
    auto probe_table = right_chunk.table_view();
    auto probe_keys = probe_table.select(right_on);
    auto [probe_match, build_match] =
        joiner.inner_join(probe_keys, std::nullopt, chunk_stream, ctx->br()->device_mr());

    cudf::column_view build_indices =
        cudf::device_span<cudf::size_type const>(*build_match);
    cudf::column_view probe_indices =
        cudf::device_span<cudf::size_type const>(*probe_match);
    // build_carrier is valid on build_stream, but chunk_stream is
    // waiting for build_stream work to be done, so running this on
    // chunk_stream is fine.
    auto result_columns = cudf::gather(
                              build_carrier,
                              build_indices,
                              cudf::out_of_bounds_policy::DONT_CHECK,
                              chunk_stream,
                              ctx->br()->device_mr()
    )
                              ->release();
    // drop key columns from probe table.
    std::vector<cudf::size_type> to_keep;
    std::ranges::copy_if(
        std::ranges::iota_view(0, probe_table.num_columns()),
        std::back_inserter(to_keep),
        [&](auto i) { return std::ranges::find(right_on, i) == right_on.end(); }
    );
    std::ranges::move(
        cudf::gather(
            probe_table.select(to_keep),
            probe_indices,
            cudf::out_of_bounds_policy::DONT_CHECK,
            chunk_stream,
            ctx->br()->device_mr()
        )
            ->release(),
        std::back_inserter(result_columns)
    );
    // Deallocation of the join indices will happen on build_stream, so add stream dep
    // This also ensure deallocation of the hash_join object waits for completion.
    cuda_stream_join(build_stream, chunk_stream, &event);
    return streaming::to_message(
        sequence,
        std::make_unique<streaming::TableChunk>(
            std::make_unique<cudf::table>(std::move(result_columns)), chunk_stream
        )
    );
}

streaming::Node inner_join_broadcast(
    std::shared_ptr<streaming::Context> ctx,
    // We will always choose left as build table and do "broadcast" joins
    std::shared_ptr<streaming::Channel> left,
    std::shared_ptr<streaming::Channel> right,
    std::shared_ptr<streaming::Channel> ch_out,
    std::vector<cudf::size_type> left_on,
    std::vector<cudf::size_type> right_on,
    OpID tag,
    KeepKeys keep_keys
) {
    streaming::ShutdownAtExit c{left, right, ch_out};
    co_await ctx->executor()->schedule();
    ctx->comm()->logger().print("Inner broadcast join ", static_cast<int>(tag));
    auto build_table = to_device(
        ctx,
        (co_await broadcast(ctx, left, tag, streaming::AllGather::Ordered::NO))
            .release<streaming::TableChunk>()
    );
    ctx->comm()->logger().print(
        "Build table has ", build_table.table_view().num_rows(), " rows"
    );

    auto joiner = cudf::hash_join(
        build_table.table_view().select(left_on),
        cudf::null_equality::UNEQUAL,
        build_table.stream()
    );
    CudaEvent build_event;
    build_event.record(build_table.stream());
    cudf::table_view build_carrier;
    if (keep_keys == KeepKeys::YES) {
        build_carrier = build_table.table_view();
    } else {
        std::vector<cudf::size_type> to_keep;
        std::ranges::copy_if(
            std::ranges::iota_view(0, build_table.table_view().num_columns()),
            std::back_inserter(to_keep),
            [&](auto i) { return std::ranges::find(left_on, i) == left_on.end(); }
        );
        build_carrier = build_table.table_view().select(to_keep);
    }
    std::size_t sequence = 0;
    while (!ch_out->is_shutdown()) {
        auto right_msg = co_await right->receive();
        if (right_msg.empty()) {
            break;
        }
        co_await ch_out->send(inner_join_chunk(
            ctx,
            right_msg.release<streaming::TableChunk>(),
            sequence++,
            joiner,
            build_carrier,
            right_on,
            build_table.stream(),
            &build_event
        ));
    }

    co_await ch_out->drain(ctx->executor());
}

streaming::Node inner_join_shuffle(
    std::shared_ptr<streaming::Context> ctx,
    std::shared_ptr<streaming::Channel> left,
    std::shared_ptr<streaming::Channel> right,
    std::shared_ptr<streaming::Channel> ch_out,
    std::vector<cudf::size_type> left_on,
    std::vector<cudf::size_type> right_on,
    KeepKeys keep_keys
) {
    streaming::ShutdownAtExit c{left, right, ch_out};
    ctx->comm()->logger().print("Inner shuffle join");
    co_await ctx->executor()->schedule();
    CudaEvent build_event;
    while (!ch_out->is_shutdown()) {
        // Requirement: two shuffles kick out partitions in the same order
        auto left_msg = co_await left->receive();
        auto right_msg = co_await right->receive();
        if (left_msg.empty()) {
            RAPIDSMPF_EXPECTS(
                right_msg.empty(), "Left does not have same number of partitions as right"
            );
            break;
        }
        RAPIDSMPF_EXPECTS(
            left_msg.sequence_number() == right_msg.sequence_number(),
            "Mismatching sequence numbers"
        );
        // TODO: currently always using left as build table.
        auto build_chunk = to_device(ctx, left_msg.release<streaming::TableChunk>());
        auto build_stream = build_chunk.stream();
        auto joiner = cudf::hash_join(
            build_chunk.table_view().select(left_on),
            cudf::null_equality::UNEQUAL,
            build_stream
        );
        build_event.record(build_stream);
        cudf::table_view build_carrier;
        if (keep_keys == KeepKeys::YES) {
            build_carrier = build_chunk.table_view();
        } else {
            std::vector<cudf::size_type> to_keep;
            std::ranges::copy_if(
                std::ranges::iota_view(0, build_chunk.table_view().num_columns()),
                std::back_inserter(to_keep),
                [&](auto i) { return std::ranges::find(left_on, i) == left_on.end(); }
            );
            build_carrier = build_chunk.table_view().select(to_keep);
        }
        co_await ch_out->send(inner_join_chunk(
            ctx,
            right_msg.release<streaming::TableChunk>(),
            left_msg.sequence_number(),
            joiner,
            build_carrier,
            right_on,
            build_stream,
            &build_event
        ));
    }
    co_await ch_out->drain(ctx->executor());
}

/**
 * @brief Perform a left semi join between two tables, broadcasting the left table to all
 * ranks.
 *
 * @param ctx Streaming context
 * @param left Channel of `TableChunk`s used as the broadcasted build side. This table is
 * broadcasted to all ranks.
 * @param right Channel of `TableChunk`s joined in turn against the build side. This table
 * is required to be shuffled / hash-partitioned.
 * @param ch_out Output channel of `TableChunk`s.
 * @param left_on Column indices of the keys in the left table.
 * @param right_on Column indices of the keys in the right table.
 * @param tag Disambiguating tag for the broadcast of the left table.
 * @param keep_keys Does the result contain the key columns, or only "carrier" value
 * columns
 * @return Coroutine representing the completion of the join.
 *
 * @note This implementation assumes that:
 * - `left` is small and fits in memory
 * - `right` is shuffled / hash-partitioned
 * It doesn't implement build table reuse across chunks of `left`, because we assume that
 * `right` is too large to broadcast.
 */
streaming::Node left_semi_join_broadcast_left(
    std::shared_ptr<streaming::Context> ctx,
    std::shared_ptr<streaming::Channel> left,
    std::shared_ptr<streaming::Channel> right,
    std::shared_ptr<streaming::Channel> ch_out,
    std::vector<cudf::size_type> left_on,
    std::vector<cudf::size_type> right_on,
    OpID tag,
    [[maybe_unused]] KeepKeys keep_keys
) {
    streaming::ShutdownAtExit c{left, right, ch_out};
    co_await ctx->executor()->schedule();
    ctx->comm()->logger().print("Inner broadcast join ", static_cast<int>(tag));
    auto left_table = to_device(
        ctx, (co_await broadcast(ctx, left, tag)).release<streaming::TableChunk>()
    );
    ctx->comm()->logger().print(
        "Left (probe) table has ", left_table.table_view().num_rows(), " rows"
    );
    CudaEvent left_event;
    left_event.record(left_table.stream());

    std::size_t sequence = 0;
    while (true) {
        auto right_msg = co_await right->receive();
        if (right_msg.empty()) {
            break;
        }
        co_await ch_out->send(semi_join_chunk(
            ctx,
            left_table,
            right_msg.release<streaming::TableChunk>(),
            left_on,
            right_on,
            sequence++,
            &left_event
        ));
    }

    co_await ch_out->drain(ctx->executor());
}

streaming::Node left_semi_join_shuffle(
    std::shared_ptr<streaming::Context> ctx,
    std::shared_ptr<streaming::Channel> left,
    std::shared_ptr<streaming::Channel> right,
    std::shared_ptr<streaming::Channel> ch_out,
    std::vector<cudf::size_type> left_on,
    std::vector<cudf::size_type> right_on
) {
    streaming::ShutdownAtExit c{left, right, ch_out};
    ctx->comm()->logger().print("Shuffle left semi join");

    co_await ctx->executor()->schedule();

    while (true) {
        // Requirement: two shuffles kick out partitions in the same order
        auto left_msg = co_await left->receive();
        auto right_msg = co_await right->receive();

        // We don't have any dependencies across chunks, so make an event per chunk pair.
        CudaEvent left_event;
        left_event.record(left_msg.release<streaming::TableChunk>().stream());

        if (left_msg.empty()) {
            RAPIDSMPF_EXPECTS(
                right_msg.empty(), "Left does not have same number of partitions as right"
            );
            break;
        }
        RAPIDSMPF_EXPECTS(
            left_msg.sequence_number() == right_msg.sequence_number(),
            "Mismatching sequence numbers"
        );

        co_await ch_out->send(semi_join_chunk(
            ctx,
            left_msg.release<streaming::TableChunk>(),
            right_msg.release<streaming::TableChunk>(),
            left_on,
            right_on,
            left_msg.sequence_number(),
            &left_event
        ));
    }
}

streaming::Node shuffle(
    std::shared_ptr<streaming::Context> ctx,
    std::shared_ptr<streaming::Channel> ch_in,
    std::shared_ptr<streaming::Channel> ch_out,
    std::vector<cudf::size_type> keys,
    std::uint32_t num_partitions,
    OpID tag
) {
    streaming::ShutdownAtExit c{ch_in, ch_out};
    co_await ctx->executor()->schedule();
    ctx->comm()->logger().print("Shuffle ", static_cast<int>(tag));
    streaming::ShufflerAsync shuffler(ctx, tag, num_partitions);
    while (true) {
        auto msg = co_await ch_in->receive();
        if (msg.empty()) {
            ctx->comm()->logger().print("Shuffle: no more input");
            break;
        }
        auto chunk = to_device(ctx, msg.release<streaming::TableChunk>());
        auto packed = partition_and_pack(
            chunk.table_view(),
            keys,
            static_cast<int>(num_partitions),
            cudf::hash_id::HASH_MURMUR3,
            0,
            chunk.stream(),
            ctx->br().get(),
            ctx->statistics()
        );
        shuffler.insert(std::move(packed));
    }
    co_await shuffler.insert_finished();
    for (auto pid : shuffler.local_partitions()) {
        auto packed_data = co_await shuffler.extract_async(pid);
        RAPIDSMPF_EXPECTS(packed_data.has_value(), "Partition already extracted");
        auto stream = ctx->br()->stream_pool().get_stream();
        co_await ch_out->send(
            streaming::to_message(
                pid,
                std::make_unique<streaming::TableChunk>(
                    unpack_and_concat(
                        unspill_partitions(
                            std::move(*packed_data),
                            ctx->br().get(),
                            true,
                            ctx->statistics()
                        ),
                        stream,
                        ctx->br().get(),
                        ctx->statistics()
                    ),
                    stream
                )
            )
        );
    }
    co_await ch_out->drain(ctx->executor());
}

}  // namespace rapidsmpf::ndsh
